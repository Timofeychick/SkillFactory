{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем библиотеку для выполнения HTTP-запросов в интернет\n",
    "import requests\n",
    "\n",
    "# Читаем текстовый файл по url-ссылке\n",
    "data = requests.get(\"https://raw.githubusercontent.com/SkillfactoryDS/Datasets/master/war_peace_processed.txt\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.\n",
    "\n",
    "Давайте введем понятие частоты употребления отдельного слова в документе (`term frequency`, или `tf`). В нашем случае речь идёт не о документах, а о главах книги (выше мы писали, что в текстовом документе главы разделяются строкой '[new chapter]').\n",
    "\n",
    "Формула для вычисления `term frequency` для слова `word`:\n",
    "$$ tf_{word, chapter} = \\frac {n_{word, chapter}} {n_{chapter}}$$\n",
    "\n",
    "где\n",
    "* ${n_{word, chapter}}$ - сколько раз слово `word` встречается в главе `chapter`,\n",
    "* $n_{chapter}$ - количество слов в главе `chapter`.\n",
    "\n",
    "\n",
    "Например, слово `\"гостья\"` употребляется в 15-ой главе 10 раз (${n_{word, chapter}}$).(кстати, главы у нас нумеруются с 0). Общее количество слов в тексте 15-ой главы - 1359 ($n_{chapter}$). Тогда:\n",
    "\n",
    "$$ tf_{гостья, 15} = \\frac{10}{1359} \\approx 0.007358$$\n",
    "\n",
    "**Задание:**\n",
    "\n",
    "Напишите программу, которая позволит получать частоту употребления любого заданного слова `target_word` в заданной главе `target_chapter`.\n",
    "\n",
    "**Дополнительное требование:**\n",
    "\n",
    "*Пострайтесь сделать программу максимально обобщенной. То есть желательно рассчитать характеристику `tf` для всех слов из каждой главы, чтобы впоследствии не было необходимости производить вычисления снова.*\n",
    "\n",
    "**Подсказка:**\n",
    "\n",
    "*Для этого вы можете для каждой главы создать словарь, ключами которого являются слова, а значения - частота употребления этого слова в этой главе*\n",
    "\n",
    "**Протестируйте работу программы на нескольких словах и главах.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', 'в', 'два', 'раза', 'короче', 'и', 'в', 'пять', 'раз', 'интереснее']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data.split('\\n')\n",
    "text.remove('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Задача №1** - решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разбиение текста на словарь со структурой: {номер главы: [список слов]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters_and_words = {} # Словарь со структурой: {номер главы: список всех слов в главе}\n",
    "words = [] # список, в который будут добавляться слова конкретной главы\n",
    "chapter_number = 0\n",
    "\n",
    "for word in text: \n",
    "    words.append(word)\n",
    "    if word == '[new chapter]':\n",
    "        words.remove('[new chapter]')\n",
    "        chapters_and_words[chapter_number] = words\n",
    "        words = []\n",
    "        chapter_number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Составление частотного словаря со структурой: {номер главы: [[слово, частота встречаемости], [слово, частота встречаемости]...]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_frequancy = {} # частотынй словарь со структурой: {номер главы: [[слово, частота встречаеомости в главе], [слово, частота встречаеомости в главе]...]}\n",
    "frequancy_list = []\n",
    "\n",
    "for chapter in chapters_and_words:                                  # Берем список слов соответсвующий каждой главе\n",
    "    unique_words = set(chapters_and_words[chapter])                 # Находим какие слова встречаются в главе (без повторений)\n",
    "    for word in (unique_words):                                     # Берем слово из множества уникальных слов \n",
    "        quantity = chapters_and_words[chapter].count(word)          # Считаем сколько раз это слово встретилось в главе\n",
    "        frequancy = quantity / len(chapters_and_words[chapter])     # Находим частототу встречаемости \n",
    "        word_and_freq = [word, frequancy]                           # Добавляем в список слово и его частоту\n",
    "        frequancy_list.append(word_and_freq)                        # Заносим данные в список для частотного словаря                                                        \n",
    "\n",
    "    words_frequancy[chapter] = frequancy_list                       # Добавляем список с парами [слово, частота] в частотный словарь по номеру главы \n",
    "    frequancy_list = []                                             # Обнуляем добавляемый список для записи новых пар из следующей главы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция поиска и вывод запрашиваемых значений из блока 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_frequency(dictionary, chapter, word):                  # Обявление функции поиска частоты по названию словаря, номеру главы и искомому слову\n",
    "    target_list = dictionary[chapter]                               # Выделяем нужный список по номеру главы\n",
    "    for target in target_list:                                      # Объявляем цикл для поиска нужной пары (target)\n",
    "        if word == target[0]: return target[1]                      # Если запрашиваемое слово word совпадает с первым элементом в паре (target[0]), возвращаем частоту его встречаемости (target[1])\n",
    "\n",
    "    return ('Слово не найдено')                                     # Если поиск не дал результатов, возвращаем \"Слово не найдено\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ввод искомых параметров и вывод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Частота встречаемости слова 'гостья' из главы № 15 составляет 0.007358\n"
     ]
    }
   ],
   "source": [
    "target_word = 'гостья'\n",
    "target_chapter = 15\n",
    "\n",
    "\n",
    "print(f\"Частота встречаемости слова '{target_word}' из главы № {target_chapter} составляет {round(get_word_frequency(words_frequancy, target_chapter, target_word), 6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.\n",
    "\n",
    "Пришло время познакомиться с понятием `document frequency`.\n",
    "\n",
    "`Document frequency` (для удобства сократим до `df`) — это доля документов, в которых встречается искомое слово.\n",
    "\n",
    "Вычисляется по формуле:\n",
    "\n",
    "$$ df_{word} = \\frac{N_{word}}{N} $$\n",
    "\n",
    "где\n",
    "* $N_{word}$ - число документов (глав) содержащих слово `word`,\n",
    "* $N$ - общее число документов (глав).\n",
    "\n",
    "Объясним на примере: наш текст состоит из 171 главы ($N$), а слово `\"человек\"` встречается в 115 главах. Тогда:\n",
    "\n",
    "$$ df_{человек} = \\frac{115}{171} \\approx 0.6725$$\n",
    "\n",
    "**Задание:**\n",
    "\n",
    "Напишите программу, которая позволит вычислять document frequency для заданного слова `target_word` и выведить результат на экран.\n",
    "\n",
    "**Дополнительное требование:**\n",
    "\n",
    "*Пострайтесь сделать программу максимально обобщенной. То есть желательно рассчитать характеристику `df` для всех уникальных слов из книги, чтобы впоследствии не было необходимости производить вычисления снова.*\n",
    "\n",
    "**Подсказка:**\n",
    "*Для этого вы можете создать словарь, ключами которого являются слова из книги, а значения - доля документов, содержащих эти слова*\n",
    "\n",
    "**Протестируйте работу программы на нескольких словах**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Задача №2** - решение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем библиотеку для выполнения HTTP-запросов в интернет\n",
    "import requests\n",
    "\n",
    "# Читаем текстовый файл по url-ссылке\n",
    "data = requests.get(\"https://raw.githubusercontent.com/SkillfactoryDS/Datasets/master/war_peace_processed.txt\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', 'в', 'два', 'раза', 'короче', 'и', 'в', 'пять', 'раз', 'интереснее']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = data.split('\\n')\n",
    "text.remove('')\n",
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_collection = set(text)                          # Cоздаем множество элементов текста\n",
    "words_collection.remove('[new chapter]')              # Убираем из множества разделитель глав '[new chapter]'\n",
    "count_of_chapters = text.count('[new chapter]') + 1   # Считаем количество глав (+ 1 - потому что в начале первой главы нет '[new chapter]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set(text)                              # множество всех слов в тексте без повторений\n",
    "chapters_and_words = {}                               # Словарь со структурой: {номер главы: список всех слов в главе}\n",
    "words = []                                            # список, в который будут добавляться слова конкретной главы\n",
    "number_of_chapters = 0\n",
    "\n",
    "\n",
    "for word in text: \n",
    "    words.append(word)\n",
    "    if word == '[new chapter]':\n",
    "        words.remove('[new chapter]')\n",
    "        chapters_and_words[number_of_chapters] = words\n",
    "        words = []\n",
    "        number_of_chapters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_count = {}                                                         # Словарь со структурой: {слово: количество глав с этим словом}\n",
    "\n",
    "for word in unique_words:                                                   # Берем слово из множества unique_words\n",
    "    if word not in document_count: document_count[word] = 0                 # Добавляем слово в словарь document_count\n",
    "    for chapter in chapters_and_words:                                      # Выбираем главу из словаря chapter_and_words по номеру главы (по ключу)\n",
    "        if word in chapters_and_words[chapter]: document_count[word] +=1    # Проверяем есть ли это слово (word) в этой главе (списке) и если есть увеличиваем количество. \n",
    "                                                                            # Далее цикл переходит следующей главе, а по факту пробежки по всем главам переходит к новому слову (word)\n",
    "                                                                            # Алгоритм долгий, работает за лиеное время О большое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_frequancy = {}\n",
    "for word in document_count:\n",
    "    document_frequancy[word] = document_count.get(word) / number_of_chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля документов, в которых встречается искомое слово 'аристократ' составляет 0.01176\n"
     ]
    }
   ],
   "source": [
    "target_word = 'аристократ'\n",
    "\n",
    "if target_word not in document_frequancy : print ('Это слово не найдено')\n",
    "else: print(f\"Доля документов, в которых встречается искомое слово '{target_word}' составляет {round(document_frequancy.get(target_word), 5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3\n",
    "\n",
    "Пришло время дать разъяснения: для чего мы делали вычисления выше и что нас ждет впереди?\n",
    "\n",
    "> Если какое-то слово часто употребляется в документе, то, вероятно, этот документ что-то рассказывает о предмете/действии, описываемом этим словом. Скажем, если вы читаете книгу, в которой много раз употребляется слово `\"заяц\"`, то, вероятно, эта книга про зайцев.\n",
    "\n",
    "> Однако, если вы возьмёте слово `\"и\"`, то оно будет встречаться почти в каждой книге много раз.\n",
    "\n",
    "Таким образом, если мы хотим найти наиболее значимые слова в книге, мы, с одной стороны, хотим найти наиболее частые слова, а с другой — убрать те, которые не несут важной информации, так как встречаются везде.\n",
    "\n",
    "Такая задача хорошо решается с помощью `tf-idf` — статистической метрики для оценки важности слова в тексте. Другими словами, `tf-idf` — это «контрастность» слова в документе (насколько оно выделяется среди других слов).\n",
    "\n",
    "Формула для вычисления следующая:\n",
    "\n",
    "`tf-idf = term frequency * inverse document frequency`\n",
    "\n",
    "* `tf` — это частотность термина, которая измеряет, насколько часто термин встречается в документе.\n",
    "\n",
    "* `idf` — это обратная документная частотность термина. Она измеряет непосредственно важность термина во всём множестве документов.\n",
    "\n",
    "Чтобы получить `idf`, необходимо поделить 1 на полученную в Задании 2 документную частоту (`df`):\n",
    "\n",
    "$$idf = \\frac{1}{df}$$\n",
    "\n",
    "Мы будем использовать не сырые значения `idf`, а их логарифмы, то есть $tf * log(idf)$. Сейчас мы не будем заострять внимания на том, почему следует использовать именно логарифм — это долгий разговор. Вернемся к нему, когда будем изучать методы машинного обучения для обработки текстов. Подробнее о `tf-idf` вы можете почитать [здесь](https://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/).\n",
    "\n",
    "В качестве примера измерим `tf-idf` слова `\"анна\"` в главе 4. Слово `\"анна\"` встречается в указанной главе 7 раз, при этом в 4 главе 1060 слов, всего же слово `\"анна\"` упоминается в 32 главах из 171.\n",
    "\n",
    "Таким образом, `tf-idf` данного слова в данной главе будет равно:\n",
    "\n",
    "$$tf\\_idf_{анна, 4} = tf * log(\\frac{1}{df}) = \\frac{7}{1060} * log(\\frac {171}{32}) \\approx 0.011067$$\n",
    "\n",
    "**Примечание:** здесь используется натуральный логарифм по основанию $e$, однако в общем случае основание логарифма не имеет значения, так как характеристика `tf-idf` используется для сравнения контрастности слов между собой\n",
    "\n",
    "**Задание**:\n",
    "\n",
    "Напишите программу, которая позволяет вычислять значение `tf-idf` для заданного слова `target_word` в заданной главе `target_chapter`.\n",
    "\n",
    "**Дополнительное требование:**\n",
    "\n",
    "*Пострайтесь сделать программу максимально оптимальной. То есть желательно рассчитать характеристику `tf-idf` для всех слов из каждой главы книги, чтобы впоследствии не было необходимости производить вычисления снова.*\n",
    "\n",
    "**Протестируйте работу программы на нескольких словах и главах.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
